{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI Search ‚Äì Pipeline Setup & Dokumente hochladen\n",
    "\n",
    "In diesem Notebook bereiten wir die komplette Blob ‚ûú Indexer ‚ûú Vector Search Pipeline vor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T12:25:20.790208Z",
     "start_time": "2025-11-15T12:25:19.846320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Workshop Tools + Notebook Helfer installieren\n",
    "!uv pip install -e ../../workshop_tools\n",
    "!uv pip install python-dotenv ipywidgets"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2mUsing Python 3.13.5 environment at: /Users/oscharko/PycharmProjects/Keiko-Evolutio/demo-it-tage-2025/.venv\u001B[0m\r\n",
      "\u001B[2K\u001B[2mResolved \u001B[1m38 packages\u001B[0m \u001B[2min 246ms\u001B[0m\u001B[0m                                        \u001B[0m\r\n",
      "\u001B[2K\u001B[2mPrepared \u001B[1m1 package\u001B[0m \u001B[2min 385ms\u001B[0m\u001B[0m                                              \r\n",
      "\u001B[2mUninstalled \u001B[1m1 package\u001B[0m \u001B[2min 0.47ms\u001B[0m\u001B[0m\r\n",
      "\u001B[2K\u001B[2mInstalled \u001B[1m1 package\u001B[0m \u001B[2min 1ms\u001B[0m\u001B[0mge-2025==1.0.0 (from file:///Users\u001B[0m\r\n",
      " \u001B[33m~\u001B[39m \u001B[1mfoundry-tools-it-tage-2025\u001B[0m\u001B[2m==1.0.0 (from file:///Users/oscharko/PycharmProjects/Keiko-Evolutio/demo-it-tage-2025/tools_and_data/workshop_tools)\u001B[0m\r\n",
      "\u001B[2mUsing Python 3.13.5 environment at: /Users/oscharko/PycharmProjects/Keiko-Evolutio/demo-it-tage-2025/.venv\u001B[0m\r\n",
      "\u001B[2mAudited \u001B[1m2 packages\u001B[0m \u001B[2min 2ms\u001B[0m\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Umgebungsvariablen laden"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T12:25:20.808836Z",
     "start_time": "2025-11-15T12:25:20.803042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_path = Path('..') / '..' / '.env'\n",
    "load_dotenv(env_path)\n",
    "\n",
    "print('Konfiguration geladen:')\n",
    "print(f\"  ‚Ä¢ Storage Container: {os.getenv('FILE_STORAGE_CONTAINER_NAME')}\")\n",
    "print(f\"  ‚Ä¢ Search Index: {os.getenv('VECTOR_DB_INDEX_NAME')}\")\n",
    "print(f\"  ‚Ä¢ Search Endpoint: {os.getenv('VECTOR_DB_ENDPOINT')}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Konfiguration geladen:\n",
      "  ‚Ä¢ Storage Container: workshop-documents\n",
      "  ‚Ä¢ Search Index: workshop-documents\n",
      "  ‚Ä¢ Search Endpoint: https://search-workshop-it-tage-2025.search.windows.net/\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T12:25:20.978764Z",
     "start_time": "2025-11-15T12:25:20.850471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from foundry_tools import ensure_notebook_env\n",
    "\n",
    "ensure_notebook_env(\n",
    "    ['VECTOR_DB_ENDPOINT', 'VECTOR_DB_ADMIN_KEY', 'VECTOR_DB_INDEX_NAME', 'FILE_STORAGE_CONNECTION_STRING',\n",
    "     'FILE_STORAGE_CONTAINER_NAME', 'AZURE_OPENAI_ENDPOINT', 'AZURE_OPENAI_API_KEY',\n",
    "     'AZURE_OPENAI_EMBEDDING_DEPLOYMENT'])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Alle ben√∂tigten Environment Variablen sind gesetzt\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Helfer initialisieren"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T12:25:20.990838Z",
     "start_time": "2025-11-15T12:25:20.987451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from foundry_tools import BlobStorage, VectorSearchPipeline, VectorDB\n",
    "\n",
    "blob = BlobStorage()\n",
    "pipeline = VectorSearchPipeline()\n",
    "vector_db = VectorDB()\n",
    "\n",
    "print('Bereit!')\n",
    "print(f\"  ‚Ä¢ Blob Container: {blob.container_name}\")\n",
    "print(f\"  ‚Ä¢ Index: {vector_db.index_name}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bereit!\n",
      "  ‚Ä¢ Blob Container: workshop-documents\n",
      "  ‚Ä¢ Index: workshop-documents\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Dokumente im Blob Store pr√ºfen\n",
    "\n",
    "Pr√ºfe, welche Dokumente bereits im Blob Store liegen und welche bereits indexiert sind."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T12:25:21.392457Z",
     "start_time": "2025-11-15T12:25:21.016263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Liste alle Dokumente im Blob Store\n",
    "blobs = blob.list_files()\n",
    "\n",
    "print(f'Dokumente im Blob Store: {len(blobs)}')\n",
    "print()\n",
    "\n",
    "if not blobs:\n",
    "    print('‚ö†Ô∏è  Keine Dokumente im Blob Store gefunden!')\n",
    "    print('   Bitte lade zuerst Dokumente hoch mit:')\n",
    "    print('   python3 tools_and_data/workshop_tools/azure_tools/blob_store/upload_sample_data.py')\n",
    "else:\n",
    "    # Zeige alle Blobs\n",
    "    for idx, blob_info in enumerate(blobs, start=1):\n",
    "        name = blob_info['name']\n",
    "        size_kb = blob_info['size'] / 1024\n",
    "        last_modified = blob_info['last_modified'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        print(f\"  {idx}. {name}\")\n",
    "        print(f\"     Gr√∂√üe: {size_kb:.2f} KB\")\n",
    "        print(f\"     Letzte √Ñnderung: {last_modified}\")\n",
    "        print()\n",
    "\n",
    "    # Pr√ºfe, welche Dokumente bereits indexiert sind\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Pr√ºfe indexierte Dokumente...\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    indexed_docs = vector_db.get_indexed_documents()\n",
    "\n",
    "    if indexed_docs:\n",
    "        print(f\"\\n‚úÖ {len(indexed_docs)} Dokument(e) bereits indexiert:\")\n",
    "        for doc_uri in indexed_docs:\n",
    "            # Extract just the blob name from the URL for display\n",
    "            blob_name = doc_uri.split('/')[-1]\n",
    "            print(f\"  - {blob_name}\")\n",
    "\n",
    "        # Vergleiche mit Blob Store (using blob URLs)\n",
    "        blob_urls = set()\n",
    "        for blob_info in blobs:\n",
    "            blob_url = blob_info['url']\n",
    "            blob_urls.add(blob_url)\n",
    "\n",
    "        # Find documents that are NOT yet indexed\n",
    "        not_indexed_urls = blob_urls - set(indexed_docs)\n",
    "\n",
    "        if not_indexed_urls:\n",
    "            print(f\"\\n‚ö†Ô∏è  {len(not_indexed_urls)} Dokument(e) noch NICHT indexiert:\")\n",
    "            for url in not_indexed_urls:\n",
    "                blob_name = url.split('/')[-1]\n",
    "                print(f\"  - {blob_name}\")\n",
    "            print(\"\\n‚ûú Pipeline wird erstellt und neue Dokumente werden indexiert.\")\n",
    "            needs_indexing = True\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ Alle Dokumente aus dem Blob Store sind bereits indexiert!\")\n",
    "            print(\"   Keine Aktion erforderlich.\")\n",
    "            needs_indexing = False\n",
    "    else:\n",
    "        print(\"\\n‚ÑπÔ∏è  Noch keine Dokumente indexiert.\")\n",
    "        print(\"‚ûú Pipeline wird erstellt und alle Dokumente werden indexiert.\")\n",
    "        needs_indexing = True"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokumente im Blob Store: 1\n",
      "\n",
      "  1. workshop/GedaÃàchtnis-in-Multi-Agent-Systemen.pdf\n",
      "     Gr√∂√üe: 510.21 KB\n",
      "     Letzte √Ñnderung: 2025-11-15 11:14:58\n",
      "\n",
      "================================================================================\n",
      "Pr√ºfe indexierte Dokumente...\n",
      "================================================================================\n",
      "\n",
      "‚ÑπÔ∏è  Noch keine Dokumente indexiert.\n",
      "‚ûú Pipeline wird erstellt und alle Dokumente werden indexiert.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Pipeline erstellen (falls n√∂tig)\n",
    "\n",
    "Die Pipeline wird nur erstellt/aktualisiert, wenn neue Dokumente indexiert werden m√ºssen."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T12:25:25.508270Z",
     "start_time": "2025-11-15T12:25:21.426833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if needs_indexing:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Erstelle Pipeline neu und indexiere alle Dokumente...\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"‚ö†Ô∏è  HINWEIS: Die Pipeline wird komplett neu erstellt.\")\n",
    "    print(\"   Dies ist notwendig, um sicherzustellen, dass alle Dokumente korrekt indexiert werden.\")\n",
    "    print()\n",
    "\n",
    "    # Recreate pipeline from scratch to ensure all documents are indexed\n",
    "    pipeline.bootstrap(force_recreate=True)\n",
    "    print('Index, Data Source, Skillset und Indexer sind bereit!')\n",
    "    print('Der Indexer wurde gestartet und verarbeitet alle Dokumente.')\n",
    "else:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Pipeline-Erstellung √ºbersprungen\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Alle Dokumente sind bereits indexiert.\")\n",
    "    print(\"Wenn du die Pipeline trotzdem neu erstellen m√∂chtest, f√ºhre aus:\")\n",
    "    print(\"  pipeline.bootstrap(force_recreate=True)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Erstelle Pipeline neu und indexiere alle Dokumente...\n",
      "================================================================================\n",
      "‚ö†Ô∏è  HINWEIS: Die Pipeline wird komplett neu erstellt.\n",
      "   Dies ist notwendig, um sicherzustellen, dass alle Dokumente korrekt indexiert werden.\n",
      "\n",
      "üîß L√∂sche alte Ressourcen und erstelle Pipeline neu...\n",
      "  1/8 L√∂sche Indexer...\n",
      "      ‚úÖ Indexer gel√∂scht\n",
      "  2/8 L√∂sche Skillset...\n",
      "      ‚úÖ Skillset gel√∂scht\n",
      "  3/8 L√∂sche Data Source...\n",
      "      ‚úÖ Data Source gel√∂scht\n",
      "  4/8 L√∂sche Index...\n",
      "      ‚úÖ Index gel√∂scht\n",
      "  5/8 Index erstellen...\n",
      "      ‚ÑπÔ∏è  Index 'workshop-documents' wurde neu erstellt\n",
      "      ‚úÖ Index erstellt\n",
      "  6/8 Data Source erstellen...\n",
      "      ‚úÖ Data Source erstellt\n",
      "  7/8 Skillset erstellen...\n",
      "      ‚úÖ Skillset erstellt\n",
      "  8/8 Indexer erstellen und starten...\n",
      "      ‚úÖ Indexer erstellt und gestartet\n",
      "\n",
      "‚úÖ Pipeline erfolgreich neu erstellt!\n",
      "   Der Indexer wurde gestartet und verarbeitet jetzt die Dokumente.\n",
      "Index, Data Source, Skillset und Indexer sind bereit!\n",
      "Der Indexer wurde gestartet und verarbeitet alle Dokumente.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Indexer-Status √ºberwachen (falls Indexierung l√§uft)\n",
    "\n",
    "Wenn neue Dokumente indexiert werden, √ºberwache den Indexer-Status.\n",
    "Dieser Prozess kann **1-3 Minuten** dauern, abh√§ngig von der Anzahl und Gr√∂√üe der Dokumente.\n",
    "\n",
    "**Wichtig:** Warte, bis der Status `success` zeigt, bevor du mit dem n√§chsten Schritt fortf√§hrst!"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T12:25:35.757534Z",
     "start_time": "2025-11-15T12:25:25.544891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "if needs_indexing:\n",
    "    print('Ueberwache Indexer-Status...\\n')\n",
    "\n",
    "    for i in range(12):  # Max 2 Minuten warten (12 x 10 Sekunden)\n",
    "        status_text = pipeline.get_indexer_status()\n",
    "        print(f'[{i * 10}s] {status_text}')\n",
    "\n",
    "        # Pr√ºfe ob erfolgreich abgeschlossen\n",
    "        if 'success' in status_text.lower():\n",
    "            print('\\nIndexer erfolgreich abgeschlossen!')\n",
    "            print('Du kannst jetzt mit dem naechsten Schritt fortfahren.\\n')\n",
    "            break\n",
    "        elif 'running' in status_text.lower() or 'inprogress' in status_text.lower() or 'transientfailure' in status_text.lower():\n",
    "            print('Indexer laeuft noch... warte 10 Sekunden\\n')\n",
    "            time.sleep(10)\n",
    "        else:\n",
    "            print(f'\\nUnerwarteter Status. Bitte pruefe die Ausgabe oben.')\n",
    "            break\n",
    "    else:\n",
    "        print('\\nTimeout nach 2 Minuten.')\n",
    "        print('Der Indexer laeuft moeglicherweise noch. Pruefe den Status manuell:')\n",
    "        print('   pipeline.get_indexer_status()')\n",
    "\n",
    "    # Pr√ºfe Dokumentanzahl im Index\n",
    "    doc_count = vector_db.get_document_count()\n",
    "    print(f'\\nDokumente im Index: {doc_count}')\n",
    "\n",
    "    if doc_count == 0:\n",
    "        print('\\n‚ö†Ô∏è  WARNUNG: Index ist leer!')\n",
    "        print('   Der Indexer braucht m√∂glicherweise noch etwas Zeit.')\n",
    "        print('   Warte weitere 30 Sekunden und pr√ºfe erneut...')\n",
    "        time.sleep(30)\n",
    "        doc_count = vector_db.get_document_count()\n",
    "        print(f'   Dokumente im Index: {doc_count}')\n",
    "\n",
    "        if doc_count > 0:\n",
    "            print(f'\\n‚úÖ Index enth√§lt jetzt {doc_count} Dokumente!')\n",
    "        else:\n",
    "            print('\\n‚ùå Index ist immer noch leer. Bitte pr√ºfe die Indexer-Logs.')\n",
    "    else:\n",
    "        print(f'\\n‚úÖ Index enth√§lt {doc_count} Dokumente!')\n",
    "else:\n",
    "    print('Indexer-√úberwachung √ºbersprungen (keine neuen Dokumente).')\n",
    "\n",
    "    # Zeige aktuelle Dokumentanzahl\n",
    "    doc_count = vector_db.get_document_count()\n",
    "    print(f'\\n‚úÖ Index enth√§lt {doc_count} Dokumente!')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ueberwache Indexer-Status...\n",
      "\n",
      "[0s] Status: running\n",
      "Indexer laeuft noch... warte 10 Sekunden\n",
      "\n",
      "[10s] Status: success\n",
      "Items: 1 processed, 0 failed\n",
      "\n",
      "Indexer erfolgreich abgeschlossen!\n",
      "Du kannst jetzt mit dem naechsten Schritt fortfahren.\n",
      "\n",
      "\n",
      "Dokumente im Index: 35\n",
      "\n",
      "‚úÖ Index enth√§lt 35 Dokumente!\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. BM25 (klassische Volltextsuche)\n",
    "\n",
    "BM25 ist der Standard-Ranker im \"klassischen\" Modus von Azure AI Search.\n",
    "\n",
    "**Kernidee:**\n",
    "- `queryType=full` oder `simple`\n",
    "- Kein `semanticConfiguration`, kein `vectorQueries`\n",
    "- Ranking prim√§r √ºber BM25 + evtl. Scoring Profile\n",
    "- Sucht nach exakten W√∂rtern/Begriffen im Text\n",
    "- Keine Embeddings n√∂tig, daher keine Rate-Limit-Probleme\n",
    "\n",
    "**Beispiel-Query:** \"Azure AI Foundry\""
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T12:25:36.041465Z",
     "start_time": "2025-11-15T12:25:35.791215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os\n",
    "\n",
    "# Search Client initialisieren\n",
    "search_client = SearchClient(\n",
    "    endpoint=os.getenv(\"VECTOR_DB_ENDPOINT\"),\n",
    "    index_name=os.getenv(\"VECTOR_DB_INDEX_NAME\"),\n",
    "    credential=AzureKeyCredential(os.getenv(\"VECTOR_DB_ADMIN_KEY\"))\n",
    ")\n",
    "\n",
    "# BM25 Suche\n",
    "query = \"Plastizit√§t\"\n",
    "results = search_client.search(\n",
    "    search_text=query,\n",
    "    query_type=\"simple\",  # oder \"full\" f√ºr erweiterte Lucene-Syntax\n",
    "    top=5,\n",
    "    select=[\"chunk_id\", \"title\", \"content\"]\n",
    ")\n",
    "\n",
    "print(f\"BM25 Suche nach: '{query}'\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, doc in enumerate(results, start=1):\n",
    "    print(f\"\\n{i}. {doc.get('title')}\")\n",
    "    print(f\"   Score: {doc.get('@search.score'):.2f}\")\n",
    "    snippet = doc.get('content', '')[:200]\n",
    "    print(f\"   {snippet}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ERKL√ÑRUNG:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "BM25 (Best Matching 25) ist ein klassischer Ranking-Algorithmus f√ºr Volltextsuche.\n",
    "\n",
    "Funktionsweise:\n",
    "- Analysiert die H√§ufigkeit von Begriffen im Dokument (Term Frequency)\n",
    "- Ber√ºcksichtigt die Seltenheit von Begriffen im gesamten Index (Inverse Document Frequency)\n",
    "- Normalisiert nach Dokumentl√§nge\n",
    "\n",
    "Vorteile:\n",
    "- Sehr schnell und effizient\n",
    "- Gut f√ºr exakte Begriffe und spezifische Suchen\n",
    "- Keine Embeddings n√∂tig (kein Rate-Limit-Problem)\n",
    "\n",
    "Nachteile:\n",
    "- Versteht keine Synonyme oder semantische √Ñhnlichkeit\n",
    "- Findet nur Dokumente mit exakten Keywords\n",
    "- Keine Sprachverst√§ndnis\n",
    "\n",
    "Verwendung: queryType=\"simple\" oder \"full\" (Lucene-Syntax)\n",
    "\"\"\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 Suche nach: 'Plastizit√§t'\n",
      "\n",
      "================================================================================\n",
      "\n",
      "1. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf\n",
      "   Score: 1.81\n",
      "   Informationen, \n",
      "\n",
      "aber nur etwa T` Bits schaffen es ins Bewusstsein. Diese massive Filterung verhindert, dass wir im \n",
      "\n",
      "Datenmeer untergehen. \n",
      "\n",
      "Biologische Systeme haben daf√ºr ausgekl√ºgelte Mechanismen ...\n",
      "\n",
      "2. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf\n",
      "   Score: 1.59\n",
      "   bestimmte Muster sieht, desto \n",
      "\n",
      "st√§rker werden die entsprechenden Verbindungen ‚Äì ein direktes technisches Analogon zur biologischen \n",
      "\n",
      "Plastizit√§t. \n",
      "\n",
      "In Multi-Agent-Systemen implementieren wir Plastizi...\n",
      "\n",
      "3. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf\n",
      "   Score: 1.28\n",
      "   (WpHG) fordert Aufzeichnungen √ºber \n",
      "\n",
      "Anlageberatungen, und Basel III/CRD IV setzen strenge Anforderungen an Risikobewertungen. \n",
      "\n",
      "Der Agent muss also nicht nur ‚Äûsich erinnern‚Äú, sondern auch ‚Äûvergessen ...\n",
      "\n",
      "4. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf\n",
      "   Score: 0.43\n",
      "   Plastizit√§t bedeutet, dass Erinnerungen ver√§nderbar sind: Sie passen sich neuen Erfahrungen an, \n",
      "\n",
      "wachsen mit, verblassen oder verschmelzen. Diese Eigenschaft erm√∂glicht es biologischen Systemen, \n",
      "\n",
      "au...\n",
      "\n",
      "================================================================================\n",
      "ERKL√ÑRUNG:\n",
      "================================================================================\n",
      "\n",
      "BM25 (Best Matching 25) ist ein klassischer Ranking-Algorithmus f√ºr Volltextsuche.\n",
      "\n",
      "Funktionsweise:\n",
      "- Analysiert die H√§ufigkeit von Begriffen im Dokument (Term Frequency)\n",
      "- Ber√ºcksichtigt die Seltenheit von Begriffen im gesamten Index (Inverse Document Frequency)\n",
      "- Normalisiert nach Dokumentl√§nge\n",
      "\n",
      "Vorteile:\n",
      "- Sehr schnell und effizient\n",
      "- Gut f√ºr exakte Begriffe und spezifische Suchen\n",
      "- Keine Embeddings n√∂tig (kein Rate-Limit-Problem)\n",
      "\n",
      "Nachteile:\n",
      "- Versteht keine Synonyme oder semantische √Ñhnlichkeit\n",
      "- Findet nur Dokumente mit exakten Keywords\n",
      "- Keine Sprachverst√§ndnis\n",
      "\n",
      "Verwendung: queryType=\"simple\" oder \"full\" (Lucene-Syntax)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 8. Semantische Suche (Semantic Search)\n",
    "\n",
    "Semantische Suche nutzt BM25 als Recall-Schicht + zus√§tzlichen semantischen Ranker.\n",
    "\n",
    "**Kernidee:**\n",
    "- `queryType=\"semantic\"`\n",
    "- `semanticConfiguration=\"workshop-semantic-config\"`\n",
    "- Optional: `answers`, `captions`, `queryLanguage`\n",
    "- BM25 holt relevante Dokumente, semantischer Ranker ordnet sie nach Bedeutung\n",
    "\n",
    "**Vorteile:**\n",
    "- Versteht Kontext und Bedeutung, nicht nur Keywords\n",
    "- Besonders gut f√ºr nat√ºrlichsprachige Fragen\n",
    "- Funktioniert auch ohne Vektor-Embeddings\n",
    "\n",
    "**Beispiel-Query:** \"Wie kann ich agentenbasierte KI-Workflows √ºberwachen?\""
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T12:25:36.458973Z",
     "start_time": "2025-11-15T12:25:36.045265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os\n",
    "\n",
    "# Search Client initialisieren\n",
    "search_client = SearchClient(\n",
    "    endpoint=os.getenv(\"VECTOR_DB_ENDPOINT\"),\n",
    "    index_name=os.getenv(\"VECTOR_DB_INDEX_NAME\"),\n",
    "    credential=AzureKeyCredential(os.getenv(\"VECTOR_DB_ADMIN_KEY\"))\n",
    ")\n",
    "\n",
    "# Semantische Suche\n",
    "query = \"Was ist Plastizit√§t?\"\n",
    "results = search_client.search(\n",
    "    search_text=query,\n",
    "    query_type=\"semantic\",\n",
    "    semantic_configuration_name=\"workshop-semantic-config\",\n",
    "    top=5,\n",
    "    select=[\"chunk_id\", \"title\", \"content\"]\n",
    ")\n",
    "\n",
    "print(f\"Semantische Suche nach: '{query}'\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, doc in enumerate(results, start=1):\n",
    "    print(f\"\\n{i}. {doc.get('title')}\")\n",
    "    print(f\"   Reranker Score: {doc.get('@search.reranker_score', 'N/A')}\")\n",
    "    print(f\"   BM25 Score: {doc.get('@search.score'):.2f}\")\n",
    "\n",
    "    snippet = doc.get('content', '')[:200]\n",
    "    print(f\"   {snippet}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ERKL√ÑRUNG:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "Der semantische Ranker funktioniert in 2 Schritten:\n",
    "\n",
    "1. BM25 Recall: Findet relevante Dokumente (wie in Abschnitt 7)\n",
    "2. Semantic Re-Ranking: Bewertet die Ergebnisse nach semantischer Relevanz\n",
    "\n",
    "Der Reranker Score ist h√∂her als der BM25 Score, weil er die semantische\n",
    "Bedeutung der Frage versteht, nicht nur Keywords.\n",
    "\n",
    "HINWEIS: Die Python SDK unterst√ºtzt 'answers' und 'captions' nicht vollst√§ndig.\n",
    "Diese Features sind haupts√§chlich √ºber die REST API verf√ºgbar:\n",
    "- answers: Extrahiert direkte Antworten aus den Dokumenten\n",
    "- captions: Zeigt relevante Textausschnitte mit Highlights\n",
    "\"\"\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantische Suche nach: 'Was ist Plastizit√§t?'\n",
      "\n",
      "================================================================================\n",
      "\n",
      "1. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf\n",
      "   Reranker Score: 3.36470627784729\n",
      "   BM25 Score: 0.43\n",
      "   Plastizit√§t bedeutet, dass Erinnerungen ver√§nderbar sind: Sie passen sich neuen Erfahrungen an, \n",
      "\n",
      "wachsen mit, verblassen oder verschmelzen. Diese Eigenschaft erm√∂glicht es biologischen Systemen, \n",
      "\n",
      "au...\n",
      "\n",
      "2. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf\n",
      "   Reranker Score: 2.966282844543457\n",
      "   BM25 Score: 1.81\n",
      "   Informationen, \n",
      "\n",
      "aber nur etwa T` Bits schaffen es ins Bewusstsein. Diese massive Filterung verhindert, dass wir im \n",
      "\n",
      "Datenmeer untergehen. \n",
      "\n",
      "Biologische Systeme haben daf√ºr ausgekl√ºgelte Mechanismen ...\n",
      "\n",
      "3. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf\n",
      "   Reranker Score: 2.5090177059173584\n",
      "   BM25 Score: 2.01\n",
      "   bestimmte Muster sieht, desto \n",
      "\n",
      "st√§rker werden die entsprechenden Verbindungen ‚Äì ein direktes technisches Analogon zur biologischen \n",
      "\n",
      "Plastizit√§t. \n",
      "\n",
      "In Multi-Agent-Systemen implementieren wir Plastizi...\n",
      "\n",
      "4. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf\n",
      "   Reranker Score: 2.2102887630462646\n",
      "   BM25 Score: 1.28\n",
      "   (WpHG) fordert Aufzeichnungen √ºber \n",
      "\n",
      "Anlageberatungen, und Basel III/CRD IV setzen strenge Anforderungen an Risikobewertungen. \n",
      "\n",
      "Der Agent muss also nicht nur ‚Äûsich erinnern‚Äú, sondern auch ‚Äûvergessen ...\n",
      "\n",
      "5. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf\n",
      "   Reranker Score: 1.6731793880462646\n",
      "   BM25 Score: 0.43\n",
      "   noch elegantere L√∂sungen f√ºr genau diese \n",
      "\n",
      "Probleme entwickelt ‚Äì von der selektiven Aufmerksamkeit √ºber plastische Ged√§chtnisprozesse bis hin \n",
      "\n",
      "zur Arbeitsteilung im Modell der Complementary Learning ...\n",
      "\n",
      "================================================================================\n",
      "ERKL√ÑRUNG:\n",
      "================================================================================\n",
      "\n",
      "Der semantische Ranker funktioniert in 2 Schritten:\n",
      "\n",
      "1. BM25 Recall: Findet relevante Dokumente (wie in Abschnitt 7)\n",
      "2. Semantic Re-Ranking: Bewertet die Ergebnisse nach semantischer Relevanz\n",
      "\n",
      "Der Reranker Score ist h√∂her als der BM25 Score, weil er die semantische\n",
      "Bedeutung der Frage versteht, nicht nur Keywords.\n",
      "\n",
      "HINWEIS: Die Python SDK unterst√ºtzt 'answers' und 'captions' nicht vollst√§ndig.\n",
      "Diese Features sind haupts√§chlich √ºber die REST API verf√ºgbar:\n",
      "- answers: Extrahiert direkte Antworten aus den Dokumenten\n",
      "- captions: Zeigt relevante Textausschnitte mit Highlights\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 9. Vektorbasierte semantische Suche (reines Vektor-Retrieval)\n",
    "\n",
    "Reine Vektor-Suche verwendet nur Embeddings, kein BM25.\n",
    "\n",
    "**Kernidee:**\n",
    "- `vector_queries` mit `VectorizedQuery`\n",
    "- Kein `search_text` (oder `None`)\n",
    "- Sucht √ºber das `contentVector` Feld\n",
    "- Verwendet HNSW-Algorithmus f√ºr schnelle Nearest-Neighbor-Suche\n",
    "\n",
    "**Unterschied zu Abschnitt 8:**\n",
    "- Abschnitt 8: BM25 + semantischer Ranker (kein Vektor)\n",
    "- Abschnitt 9: Reines Vektor-Retrieval (HNSW)\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T12:26:29.433382Z",
     "start_time": "2025-11-15T12:25:36.462263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '../../workshop_tools')\n",
    "from foundry_tools import VectorDB\n",
    "\n",
    "# VectorDB f√ºr Embedding-Generierung\n",
    "vector_db = VectorDB()\n",
    "\n",
    "# Search Client\n",
    "search_client = SearchClient(\n",
    "    endpoint=os.getenv(\"VECTOR_DB_ENDPOINT\"),\n",
    "    index_name=os.getenv(\"VECTOR_DB_INDEX_NAME\"),\n",
    "    credential=AzureKeyCredential(os.getenv(\"VECTOR_DB_ADMIN_KEY\"))\n",
    ")\n",
    "\n",
    "# Query\n",
    "query = \"Was ist Plastizit√§t?\"\n",
    "\n",
    "# Generiere Embedding f√ºr die Query\n",
    "print(f\"Generiere Embedding f√ºr: '{query}'\")\n",
    "query_embedding = vector_db._embed_text(query)\n",
    "print(f\"Embedding-Dimension: {len(query_embedding)}\")\n",
    "\n",
    "# Erstelle VectorizedQuery\n",
    "vector_query = VectorizedQuery(\n",
    "    vector=query_embedding,\n",
    "    k_nearest_neighbors=5,\n",
    "    fields=\"contentVector\"\n",
    ")\n",
    "\n",
    "# Reine Vektor-Suche (kein search_text)\n",
    "results = search_client.search(\n",
    "    search_text=None,  # Kein BM25!\n",
    "    vector_queries=[vector_query],\n",
    "    select=[\"chunk_id\", \"title\", \"content\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nVektor-Suche nach: '{query}'\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, doc in enumerate(results, start=1):\n",
    "    print(f\"\\n{i}. {doc.get('title')}\")\n",
    "    print(f\"   Similarity Score: {doc.get('@search.score'):.4f}\")\n",
    "\n",
    "    snippet = doc.get('content', '')[:200]\n",
    "    print(f\"   {snippet}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ERKL√ÑRUNG:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "Vektor-Suche verwendet:\n",
    "- HNSW-Algorithmus f√ºr schnelle Nearest-Neighbor-Suche\n",
    "- Cosine Similarity f√ºr √Ñhnlichkeitsberechnung\n",
    "- Nur semantische Bedeutung, keine Keywords\n",
    "\n",
    "Der Similarity Score ist zwischen 0 und 1:\n",
    "- 1.0 = identisch\n",
    "- 0.0 = v√∂llig unterschiedlich\n",
    "\n",
    "Vorteil: Findet semantisch √§hnliche Dokumente, auch ohne exakte Keywords\n",
    "Nachteil: Kann bei sehr spezifischen Begriffen ungenau sein\n",
    "\"\"\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generiere Embedding f√ºr: 'Was ist Plastizit√§t?'\n",
      "‚ö†Ô∏è  Rate limit erreicht (429). Warte 52 Sekunden... (Versuch 1/5)\n",
      "Embedding-Dimension: 1536\n",
      "\n",
      "Vektor-Suche nach: 'Was ist Plastizit√§t?'\n",
      "\n",
      "================================================================================\n",
      "\n",
      "1. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf\n",
      "   Similarity Score: 0.7768\n",
      "   Plastizit√§t bedeutet, dass Erinnerungen ver√§nderbar sind: Sie passen sich neuen Erfahrungen an, \n",
      "\n",
      "wachsen mit, verblassen oder verschmelzen. Diese Eigenschaft erm√∂glicht es biologischen Systemen, \n",
      "\n",
      "au...\n",
      "\n",
      "2. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf\n",
      "   Similarity Score: 0.6762\n",
      "   bestimmte Muster sieht, desto \n",
      "\n",
      "st√§rker werden die entsprechenden Verbindungen ‚Äì ein direktes technisches Analogon zur biologischen \n",
      "\n",
      "Plastizit√§t. \n",
      "\n",
      "In Multi-Agent-Systemen implementieren wir Plastizi...\n",
      "\n",
      "3. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf\n",
      "   Similarity Score: 0.6685\n",
      "   Informationen, \n",
      "\n",
      "aber nur etwa T` Bits schaffen es ins Bewusstsein. Diese massive Filterung verhindert, dass wir im \n",
      "\n",
      "Datenmeer untergehen. \n",
      "\n",
      "Biologische Systeme haben daf√ºr ausgekl√ºgelte Mechanismen ...\n",
      "\n",
      "4. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf\n",
      "   Similarity Score: 0.6184\n",
      "   noch elegantere L√∂sungen f√ºr genau diese \n",
      "\n",
      "Probleme entwickelt ‚Äì von der selektiven Aufmerksamkeit √ºber plastische Ged√§chtnisprozesse bis hin \n",
      "\n",
      "zur Arbeitsteilung im Modell der Complementary Learning ...\n",
      "\n",
      "5. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf\n",
      "   Similarity Score: 0.6144\n",
      "   funktionierendes Erinnerungsverm√∂gen zeigen \n",
      "\n",
      "eindr√ºcklich: Ohne Ged√§chtnis bleibt das Individuum im Augenblick gefangen ‚Äì f√§hig zu reagieren, aber \n",
      "\n",
      "unf√§hig zu planen, zu lernen, zu handeln oder sich...\n",
      "\n",
      "================================================================================\n",
      "ERKL√ÑRUNG:\n",
      "================================================================================\n",
      "\n",
      "Vektor-Suche verwendet:\n",
      "- HNSW-Algorithmus f√ºr schnelle Nearest-Neighbor-Suche\n",
      "- Cosine Similarity f√ºr √Ñhnlichkeitsberechnung\n",
      "- Nur semantische Bedeutung, keine Keywords\n",
      "\n",
      "Der Similarity Score ist zwischen 0 und 1:\n",
      "- 1.0 = identisch\n",
      "- 0.0 = v√∂llig unterschiedlich\n",
      "\n",
      "Vorteil: Findet semantisch √§hnliche Dokumente, auch ohne exakte Keywords\n",
      "Nachteil: Kann bei sehr spezifischen Begriffen ungenau sein\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 10. Hybrid Retrieval (BM25 + Vektor)\n",
    "\n",
    "Hybrid Retrieval kombiniert das Beste aus beiden Welten:\n",
    "- **BM25**: Findet Dokumente mit exakten Keywords\n",
    "- **Vektor-Suche**: Findet semantisch √§hnliche Dokumente\n",
    "\n",
    "**Kernidee:**\n",
    "- `search_text` + `vector_queries` gemeinsam setzen\n",
    "- Azure AI Search fusioniert beide Ergebnisse automatisch (Reciprocal Rank Fusion)\n",
    "- Optional: `query_type=\"semantic\"` f√ºr zus√§tzliches Re-Ranking\n",
    "\n",
    "**Fusion-Strategie:**\n",
    "Azure verwendet Reciprocal Rank Fusion (RRF):\n",
    "- Kombiniert Rankings aus BM25 und Vektor-Suche\n",
    "- Dokumente, die in beiden Rankings hoch sind, werden bevorzugt\n",
    "- Robuster als einzelne Methoden\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T12:26:30.039998Z",
     "start_time": "2025-11-15T12:26:29.456922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '../../workshop_tools')\n",
    "from foundry_tools import VectorDB\n",
    "\n",
    "# VectorDB f√ºr Embedding-Generierung\n",
    "vector_db = VectorDB()\n",
    "\n",
    "# Search Client\n",
    "search_client = SearchClient(\n",
    "    endpoint=os.getenv(\"VECTOR_DB_ENDPOINT\"),\n",
    "    index_name=os.getenv(\"VECTOR_DB_INDEX_NAME\"),\n",
    "    credential=AzureKeyCredential(os.getenv(\"VECTOR_DB_ADMIN_KEY\"))\n",
    ")\n",
    "\n",
    "# Query\n",
    "query = \"Was ist Plastizit√§t?\"\n",
    "\n",
    "# Generiere Embedding f√ºr die Query\n",
    "print(f\"Generiere Embedding f√ºr: '{query}'\")\n",
    "query_embedding = vector_db._embed_text(query)\n",
    "\n",
    "# Erstelle VectorizedQuery\n",
    "vector_query = VectorizedQuery(\n",
    "    vector=query_embedding,\n",
    "    k_nearest_neighbors=20,  # Mehr Kandidaten f√ºr bessere Fusion\n",
    "    fields=\"contentVector\"\n",
    ")\n",
    "\n",
    "# Hybrid Search: BM25 + Vektor\n",
    "results = search_client.search(\n",
    "    search_text=query,           # BM25 Keyword-Suche\n",
    "    vector_queries=[vector_query],  # Vektor-Suche\n",
    "    top=5,\n",
    "    select=[\"chunk_id\", \"title\", \"content\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nHybrid Search (BM25 + Vektor) nach: '{query}'\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, doc in enumerate(results, start=1):\n",
    "    print(f\"\\n{i}. {doc.get('title')}\")\n",
    "    print(f\"   Hybrid Score: {doc.get('@search.score'):.4f}\")\n",
    "\n",
    "    snippet = doc.get('content', '')[:200]\n",
    "    print(f\"   {snippet}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ERKL√ÑRUNG:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "Hybrid Retrieval kombiniert:\n",
    "1. BM25: Findet Dokumente mit dem Keyword \"Plastizit√§t\"\n",
    "2. Vektor-Suche: Findet semantisch √§hnliche Dokumente\n",
    "3. Reciprocal Rank Fusion (RRF): Kombiniert beide Rankings\n",
    "\n",
    "Vorteile:\n",
    "- Robuster als einzelne Methoden\n",
    "- Findet sowohl exakte Matches als auch semantisch √§hnliche Dokumente\n",
    "- Bessere Recall-Rate\n",
    "\n",
    "Der Hybrid Score ist eine Kombination aus BM25 und Vektor-Similarity.\n",
    "\n",
    "Vergleichen Sie die Ergebnisse mit:\n",
    "- Abschnitt 7 (nur BM25)\n",
    "- Abschnitt 9 (nur Vektor)\n",
    "\"\"\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generiere Embedding f√ºr: 'Was ist Plastizit√§t?'\n",
      "\n",
      "Hybrid Search (BM25 + Vektor) nach: 'Was ist Plastizit√§t?'\n",
      "\n",
      "================================================================================\n",
      "\n",
      "1. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf\n",
      "   Hybrid Score: 0.0331\n",
      "   bestimmte Muster sieht, desto \n",
      "\n",
      "st√§rker werden die entsprechenden Verbindungen ‚Äì ein direktes technisches Analogon zur biologischen \n",
      "\n",
      "Plastizit√§t. \n",
      "\n",
      "In Multi-Agent-Systemen implementieren wir Plastizi...\n",
      "\n",
      "2. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf\n",
      "   Hybrid Score: 0.0323\n",
      "   Informationen, \n",
      "\n",
      "aber nur etwa T` Bits schaffen es ins Bewusstsein. Diese massive Filterung verhindert, dass wir im \n",
      "\n",
      "Datenmeer untergehen. \n",
      "\n",
      "Biologische Systeme haben daf√ºr ausgekl√ºgelte Mechanismen ...\n",
      "\n",
      "3. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf\n",
      "   Hybrid Score: 0.0309\n",
      "   Integration von episodischen Erinnerungen und \n",
      "\n",
      "die F√§higkeit zur dynamischen Reorganisation von Wissen, wie sie in der Natur vorzufinden sind. \n",
      "\n",
      "3.5) Die fehlende episodisch-semantische Trennung \n",
      "\n",
      "Um...\n",
      "\n",
      "4. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf\n",
      "   Hybrid Score: 0.0300\n",
      "   Plastizit√§t bedeutet, dass Erinnerungen ver√§nderbar sind: Sie passen sich neuen Erfahrungen an, \n",
      "\n",
      "wachsen mit, verblassen oder verschmelzen. Diese Eigenschaft erm√∂glicht es biologischen Systemen, \n",
      "\n",
      "au...\n",
      "\n",
      "5. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf\n",
      "   Hybrid Score: 0.0299\n",
      "   √ºber etwas verf√ºgt, was wir als echtes Ged√§chtnis \n",
      "\n",
      "bezeichnen k√∂nnen. Es erinnert sich an vergangene Gespr√§che, stellt Verbindungen zwischen \n",
      "\n",
      "verschiedenen Interaktionen her und schafft dadurch Kont...\n",
      "\n",
      "================================================================================\n",
      "ERKL√ÑRUNG:\n",
      "================================================================================\n",
      "\n",
      "Hybrid Retrieval kombiniert:\n",
      "1. BM25: Findet Dokumente mit dem Keyword \"Plastizit√§t\"\n",
      "2. Vektor-Suche: Findet semantisch √§hnliche Dokumente\n",
      "3. Reciprocal Rank Fusion (RRF): Kombiniert beide Rankings\n",
      "\n",
      "Vorteile:\n",
      "- Robuster als einzelne Methoden\n",
      "- Findet sowohl exakte Matches als auch semantisch √§hnliche Dokumente\n",
      "- Bessere Recall-Rate\n",
      "\n",
      "Der Hybrid Score ist eine Kombination aus BM25 und Vektor-Similarity.\n",
      "\n",
      "Vergleichen Sie die Ergebnisse mit:\n",
      "- Abschnitt 7 (nur BM25)\n",
      "- Abschnitt 9 (nur Vektor)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 11. Re-Ranking-Varianten (klassisch vs. semantisch vs. LLM)\n",
    "\n",
    "Re-Ranking kann auf mehreren Ebenen erfolgen:\n",
    "\n",
    "1. **Semantisches Re-Ranking in Azure AI Search**\n",
    "   - `query_type=\"semantic\"` + `semantic_configuration_name`\n",
    "   - Wirkt auf Top-N BM25-Treffer\n",
    "   - Schnell, in der Cloud\n",
    "\n",
    "2. **LLM-basiertes Re-Ranking im Agent/Backend**\n",
    "   - Hole Top-20 √ºber Hybrid/BM25/Semantic\n",
    "   - LLM (Azure OpenAI) bewertet und sortiert neu\n",
    "   - Versteht komplexe Anforderungen\n",
    "\n",
    "3. **Score-Postprocessing im Code**\n",
    "   - Einfache Heuristiken (z.B. Boost f√ºr j√ºngere Dokumente)\n",
    "   - Gesch√§ftslogik-basiertes Ranking\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T12:26:32.423435Z",
     "start_time": "2025-11-15T12:26:30.043425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from openai import AzureOpenAI\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Search Client\n",
    "search_client = SearchClient(\n",
    "    endpoint=os.getenv(\"VECTOR_DB_ENDPOINT\"),\n",
    "    index_name=os.getenv(\"VECTOR_DB_INDEX_NAME\"),\n",
    "    credential=AzureKeyCredential(os.getenv(\"VECTOR_DB_ADMIN_KEY\"))\n",
    ")\n",
    "\n",
    "# Azure OpenAI Client\n",
    "openai_client = AzureOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2024-08-01-preview\"\n",
    ")\n",
    "\n",
    "query = \"Was ist Plastizit√§t?\"\n",
    "\n",
    "print(f\"Suche nach: '{query}'\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Schritt 1: Semantische Suche (Top-20)\n",
    "print(\"\\n1. SEMANTISCHE SUCHE (Top-20)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "results = list(search_client.search(\n",
    "    search_text=query,\n",
    "    query_type=\"semantic\",\n",
    "    semantic_configuration_name=\"workshop-semantic-config\",\n",
    "    top=20,\n",
    "    select=[\"chunk_id\", \"title\", \"content\"]\n",
    "))\n",
    "\n",
    "print(f\"Gefunden: {len(results)} Dokumente\")\n",
    "for i, doc in enumerate(results[:5], start=1):\n",
    "    print(f\"{i}. {doc.get('title')} - Score: {doc.get('@search.reranker_score', 'N/A')}\")\n",
    "\n",
    "# Schritt 2: LLM-basiertes Re-Ranking\n",
    "print(\"\\n2. LLM-BASIERTES RE-RANKING\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "def format_docs_for_llm(docs):\n",
    "    \"\"\"Formatiert Dokumente f√ºr LLM-Re-Ranking.\"\"\"\n",
    "    lines = []\n",
    "    for i, d in enumerate(docs, start=1):\n",
    "        chunk_id = d.get('chunk_id', 'unknown')\n",
    "        title = d.get('title', 'Unbekannt')\n",
    "        content = d.get('content', '')[:300]  # Erste 300 Zeichen\n",
    "        lines.append(f\"[{i}] ID: {chunk_id}\\nTitel: {title}\\nInhalt: {content}...\")\n",
    "    return \"\\n\\n\".join(lines)\n",
    "\n",
    "prompt = f\"\"\"Du bist ein Experte f√ºr Informationsretrieval.\n",
    "\n",
    "Aufgabe: Re-ranke die folgenden Dokumente f√ºr die Frage: \"{query}\"\n",
    "\n",
    "Bewerte jedes Dokument nach:\n",
    "1. Relevanz zur Frage\n",
    "2. Qualit√§t der Antwort\n",
    "3. Vollst√§ndigkeit der Information\n",
    "\n",
    "Gib eine JSON-Liste mit den besten 5 Dokument-Nummern zur√ºck (beste zuerst).\n",
    "Format: {{\"ranking\": [3, 1, 7, 2, 5], \"reasoning\": \"Kurze Begr√ºndung\"}}\n",
    "\n",
    "Dokumente:\n",
    "{format_docs_for_llm(results[:10])}\n",
    "\"\"\"\n",
    "\n",
    "print(\"LLM analysiert die Dokumente...\")\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT\", \"gpt-4o-mini\"),\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Du bist ein Experte f√ºr Informationsretrieval.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature=0.0,\n",
    "    response_format={\"type\": \"json_object\"}\n",
    ")\n",
    "\n",
    "llm_result = json.loads(response.choices[0].message.content)\n",
    "print(f\"\\nLLM Ranking: {llm_result.get('ranking', [])}\")\n",
    "print(f\"Begr√ºndung: {llm_result.get('reasoning', 'N/A')}\")\n",
    "\n",
    "# Zeige re-ranked Ergebnisse\n",
    "print(\"\\n3. RE-RANKED ERGEBNISSE\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for rank, doc_num in enumerate(llm_result.get('ranking', [])[:5], start=1):\n",
    "    if doc_num <= len(results):\n",
    "        doc = results[doc_num - 1]\n",
    "        print(f\"\\n{rank}. {doc.get('title')} (Original Position: {doc_num})\")\n",
    "        print(f\"   Original Score: {doc.get('@search.reranker_score', 'N/A')}\")\n",
    "        snippet = doc.get('content', '')[:200]\n",
    "        print(f\"   {snippet}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ERKL√ÑRUNG:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "Re-Ranking-Ebenen im Vergleich:\n",
    "\n",
    "1. Semantisches Re-Ranking (Azure AI Search):\n",
    "   - Schnell, in der Cloud\n",
    "   - Basiert auf vortrainierten Modellen\n",
    "   - Gut f√ºr Standard-Anfragen\n",
    "\n",
    "2. LLM-basiertes Re-Ranking:\n",
    "   - Versteht komplexe Anforderungen\n",
    "   - Kann Kontext und Nuancen ber√ºcksichtigen\n",
    "   - Langsamer, aber pr√§ziser\n",
    "   - Kostet mehr (LLM-Aufrufe)\n",
    "\n",
    "3. Score-Postprocessing:\n",
    "   - Einfache Heuristiken (z.B. Boost f√ºr neue Dokumente)\n",
    "   - Sehr schnell\n",
    "   - Gut f√ºr gesch√§ftsspezifische Regeln\n",
    "\n",
    "Best Practice: Kombiniere mehrere Ebenen f√ºr optimale Ergebnisse!\n",
    "\"\"\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suche nach: 'Was ist Plastizit√§t?'\n",
      "================================================================================\n",
      "\n",
      "1. SEMANTISCHE SUCHE (Top-20)\n",
      "--------------------------------------------------------------------------------\n",
      "Gefunden: 20 Dokumente\n",
      "1. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf - Score: 3.36470627784729\n",
      "2. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf - Score: 2.966282844543457\n",
      "3. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf - Score: 2.5090177059173584\n",
      "4. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf - Score: 2.2102887630462646\n",
      "5. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf - Score: 1.6731793880462646\n",
      "\n",
      "2. LLM-BASIERTES RE-RANKING\n",
      "--------------------------------------------------------------------------------\n",
      "LLM analysiert die Dokumente...\n",
      "\n",
      "LLM Ranking: [1, 3, 5, 8, 4]\n",
      "Begr√ºndung: Dokument 1 bietet eine klare Definition von Plastizit√§t und erkl√§rt deren Bedeutung in biologischen Systemen, was es sehr relevant macht. Dokument 3 beschreibt die Implementierung von Plastizit√§t in Multi-Agent-Systemen und bietet technische Analogien, was die Qualit√§t der Antwort erh√∂ht. Dokument 5 diskutiert elegante L√∂sungen in Bezug auf plastische Ged√§chtnisprozesse, was die Vollst√§ndigkeit der Information unterst√ºtzt. Dokument 8 behandelt die Effizienz von Ged√§chtnisprozessen, was ebenfalls relevant ist, w√§hrend Dokument 4 zwar interessante Informationen bietet, aber weniger direkt auf die Frage eingeht.\n",
      "\n",
      "3. RE-RANKED ERGEBNISSE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf (Original Position: 1)\n",
      "   Original Score: 3.36470627784729\n",
      "   Plastizit√§t bedeutet, dass Erinnerungen ver√§nderbar sind: Sie passen sich neuen Erfahrungen an, \n",
      "\n",
      "wachsen mit, verblassen oder verschmelzen. Diese Eigenschaft erm√∂glicht es biologischen Systemen, \n",
      "\n",
      "au...\n",
      "\n",
      "2. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf (Original Position: 3)\n",
      "   Original Score: 2.5090177059173584\n",
      "   bestimmte Muster sieht, desto \n",
      "\n",
      "st√§rker werden die entsprechenden Verbindungen ‚Äì ein direktes technisches Analogon zur biologischen \n",
      "\n",
      "Plastizit√§t. \n",
      "\n",
      "In Multi-Agent-Systemen implementieren wir Plastizi...\n",
      "\n",
      "3. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf (Original Position: 5)\n",
      "   Original Score: 1.6731793880462646\n",
      "   noch elegantere L√∂sungen f√ºr genau diese \n",
      "\n",
      "Probleme entwickelt ‚Äì von der selektiven Aufmerksamkeit √ºber plastische Ged√§chtnisprozesse bis hin \n",
      "\n",
      "zur Arbeitsteilung im Modell der Complementary Learning ...\n",
      "\n",
      "4. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf (Original Position: 8)\n",
      "   Original Score: 1.0719997882843018\n",
      "   Das Gehirn beh√§lt bevorzugt Informationen, \n",
      "\n",
      "die statistisch wahrscheinlich wieder ben√∂tigt werden. Neue Erinnerungen verdr√§ngen √§hnliche alte \n",
      "\n",
      "durch retroaktive Interferenz ‚Äì ein Mechanismus, der Re...\n",
      "\n",
      "5. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf (Original Position: 4)\n",
      "   Original Score: 2.2102887630462646\n",
      "   (WpHG) fordert Aufzeichnungen √ºber \n",
      "\n",
      "Anlageberatungen, und Basel III/CRD IV setzen strenge Anforderungen an Risikobewertungen. \n",
      "\n",
      "Der Agent muss also nicht nur ‚Äûsich erinnern‚Äú, sondern auch ‚Äûvergessen ...\n",
      "\n",
      "================================================================================\n",
      "ERKL√ÑRUNG:\n",
      "================================================================================\n",
      "\n",
      "Re-Ranking-Ebenen im Vergleich:\n",
      "\n",
      "1. Semantisches Re-Ranking (Azure AI Search):\n",
      "   - Schnell, in der Cloud\n",
      "   - Basiert auf vortrainierten Modellen\n",
      "   - Gut f√ºr Standard-Anfragen\n",
      "\n",
      "2. LLM-basiertes Re-Ranking:\n",
      "   - Versteht komplexe Anforderungen\n",
      "   - Kann Kontext und Nuancen ber√ºcksichtigen\n",
      "   - Langsamer, aber pr√§ziser\n",
      "   - Kostet mehr (LLM-Aufrufe)\n",
      "\n",
      "3. Score-Postprocessing:\n",
      "   - Einfache Heuristiken (z.B. Boost f√ºr neue Dokumente)\n",
      "   - Sehr schnell\n",
      "   - Gut f√ºr gesch√§ftsspezifische Regeln\n",
      "\n",
      "Best Practice: Kombiniere mehrere Ebenen f√ºr optimale Ergebnisse!\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 12. Typische API-Kombinationen im Azure-AI-Foundry-Kontext\n",
    "\n",
    "Wenn Sie einen Agenten oder ein Tool in Azure AI Foundry bauen, der Azure AI Search nutzt,\n",
    "sieht das Backend typischerweise so aus:\n",
    "\n",
    "**Typische Szenarien:**\n",
    "\n",
    "1. **BM25 (Keyword-Suche)**\n",
    "   - `search_text` + `query_type=\"full\"`\n",
    "   - Schnell, f√ºr exakte Begriffe\n",
    "\n",
    "2. **Semantische Suche**\n",
    "   - `query_type=\"semantic\"` + `semantic_configuration_name`\n",
    "   - BM25 + semantischer Ranker\n",
    "\n",
    "3. **Hybrid Search**\n",
    "   - `search_text` + `vector_queries` + optional `query_type=\"semantic\"`\n",
    "   - Beste Recall-Rate\n",
    "\n",
    "4. **Re-Ranking**\n",
    "   - Entweder semantisch (`query_type=\"semantic\"`)\n",
    "   - Oder zus√§tzlicher LLM-Aufruf nach `client.search(...)`\n",
    "\n",
    "**Best Practice:** Erstellen Sie wiederverwendbare Funktionen f√ºr Ihre Tools!\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T12:26:33.620982Z",
     "start_time": "2025-11-15T12:26:32.445157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '../../workshop_tools')\n",
    "from foundry_tools import VectorDB\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Clients initialisieren\n",
    "vector_db = VectorDB()\n",
    "\n",
    "search_client = SearchClient(\n",
    "    endpoint=os.getenv(\"VECTOR_DB_ENDPOINT\"),\n",
    "    index_name=os.getenv(\"VECTOR_DB_INDEX_NAME\"),\n",
    "    credential=AzureKeyCredential(os.getenv(\"VECTOR_DB_ADMIN_KEY\"))\n",
    ")\n",
    "\n",
    "# Wiederverwendbare Such-Funktionen f√ºr Azure AI Foundry Tools\n",
    "\n",
    "def bm25_search(query: str, top_k: int = 10) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    BM25 Keyword-Suche.\n",
    "\n",
    "    Verwendung: F√ºr exakte Begriffe, schnelle Suche.\n",
    "    \"\"\"\n",
    "    results = search_client.search(\n",
    "        search_text=query,\n",
    "        query_type=\"full\",  # Lucene-Syntax\n",
    "        top=top_k,\n",
    "        select=[\"chunk_id\", \"title\", \"content\"]\n",
    "    )\n",
    "    return list(results)\n",
    "\n",
    "\n",
    "def semantic_search(query: str, top_k: int = 10) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Semantische Suche (BM25 + semantischer Ranker).\n",
    "\n",
    "    Verwendung: F√ºr nat√ºrlichsprachliche Fragen.\n",
    "    \"\"\"\n",
    "    results = search_client.search(\n",
    "        search_text=query,\n",
    "        query_type=\"semantic\",\n",
    "        semantic_configuration_name=\"workshop-semantic-config\",\n",
    "        top=top_k,\n",
    "        select=[\"chunk_id\", \"title\", \"content\"]\n",
    "    )\n",
    "    return list(results)\n",
    "\n",
    "\n",
    "def hybrid_search(query: str, top_k: int = 10) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Hybrid Search (BM25 + Vektor + semantischer Ranker).\n",
    "\n",
    "    Verwendung: Beste Recall-Rate, empfohlen f√ºr Production.\n",
    "    \"\"\"\n",
    "    # Generiere Embedding\n",
    "    embedding = vector_db._embed_text(query)\n",
    "\n",
    "    # Erstelle VectorizedQuery\n",
    "    vector_query = VectorizedQuery(\n",
    "        vector=embedding,\n",
    "        k_nearest_neighbors=top_k * 2,  # Mehr Kandidaten f√ºr bessere Fusion\n",
    "        fields=\"contentVector\"\n",
    "    )\n",
    "\n",
    "    # Hybrid Search mit semantischem Re-Ranking\n",
    "    results = search_client.search(\n",
    "        search_text=query,\n",
    "        query_type=\"semantic\",\n",
    "        semantic_configuration_name=\"workshop-semantic-config\",\n",
    "        vector_queries=[vector_query],\n",
    "        top=top_k,\n",
    "        select=[\"chunk_id\", \"title\", \"content\"]\n",
    "    )\n",
    "\n",
    "    return list(results)\n",
    "\n",
    "\n",
    "def vector_only_search(query: str, top_k: int = 10) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Reine Vektor-Suche (kein BM25).\n",
    "\n",
    "    Verwendung: F√ºr semantische √Ñhnlichkeit ohne Keywords.\n",
    "    \"\"\"\n",
    "    # Generiere Embedding\n",
    "    embedding = vector_db._embed_text(query)\n",
    "\n",
    "    # Erstelle VectorizedQuery\n",
    "    vector_query = VectorizedQuery(\n",
    "        vector=embedding,\n",
    "        k_nearest_neighbors=top_k,\n",
    "        fields=\"contentVector\"\n",
    "    )\n",
    "\n",
    "    # Reine Vektor-Suche\n",
    "    results = search_client.search(\n",
    "        search_text=None,  # Kein BM25\n",
    "        vector_queries=[vector_query],\n",
    "        top=top_k,\n",
    "        select=[\"chunk_id\", \"title\", \"content\"]\n",
    "    )\n",
    "\n",
    "    return list(results)\n",
    "\n",
    "\n",
    "# Beispiel: Verwendung in einem Azure AI Foundry Tool\n",
    "print(\"BEISPIEL: Wiederverwendbare Such-Funktionen\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "query = \"Was ist Plastizit√§t?\"\n",
    "\n",
    "print(f\"\\nQuery: '{query}'\\n\")\n",
    "\n",
    "# 1. BM25 Search\n",
    "print(\"1. BM25 SEARCH\")\n",
    "print(\"-\" * 80)\n",
    "bm25_results = bm25_search(query, top_k=3)\n",
    "for i, doc in enumerate(bm25_results, start=1):\n",
    "    print(f\"{i}. {doc.get('title')} - Score: {doc.get('@search.score'):.2f}\")\n",
    "\n",
    "# 2. Semantic Search\n",
    "print(\"\\n2. SEMANTIC SEARCH\")\n",
    "print(\"-\" * 80)\n",
    "semantic_results = semantic_search(query, top_k=3)\n",
    "for i, doc in enumerate(semantic_results, start=1):\n",
    "    print(f\"{i}. {doc.get('title')} - Reranker Score: {doc.get('@search.reranker_score', 'N/A')}\")\n",
    "\n",
    "# 3. Hybrid Search (EMPFOHLEN)\n",
    "print(\"\\n3. HYBRID SEARCH (EMPFOHLEN)\")\n",
    "print(\"-\" * 80)\n",
    "hybrid_results = hybrid_search(query, top_k=3)\n",
    "for i, doc in enumerate(hybrid_results, start=1):\n",
    "    print(f\"{i}. {doc.get('title')} - Reranker Score: {doc.get('@search.reranker_score', 'N/A')}\")\n",
    "\n",
    "# 4. Vector Only Search\n",
    "print(\"\\n4. VECTOR ONLY SEARCH\")\n",
    "print(\"-\" * 80)\n",
    "vector_results = vector_only_search(query, top_k=3)\n",
    "for i, doc in enumerate(vector_results, start=1):\n",
    "    print(f\"{i}. {doc.get('title')} - Similarity: {doc.get('@search.score'):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EMPFEHLUNG F√úR AZURE AI FOUNDRY TOOLS:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "F√ºr Production-Agenten empfehlen wir:\n",
    "\n",
    "1. Standard-Tool: hybrid_search()\n",
    "   - Beste Recall-Rate\n",
    "   - Kombiniert BM25, Vektor und semantischen Ranker\n",
    "   - Robust f√ºr verschiedene Query-Typen\n",
    "\n",
    "2. Spezial-Tools:\n",
    "   - bm25_search(): F√ºr exakte Begriffe (z.B. Produktnummern)\n",
    "   - vector_only_search(): F√ºr semantische √Ñhnlichkeit\n",
    "   - semantic_search(): F√ºr nat√ºrlichsprachliche Fragen\n",
    "\n",
    "3. Tool-Definition in Azure AI Foundry:\n",
    "   {\n",
    "     \"name\": \"search_knowledge_base\",\n",
    "     \"description\": \"Durchsucht die Wissensdatenbank\",\n",
    "     \"parameters\": {\n",
    "       \"query\": {\"type\": \"string\", \"description\": \"Suchanfrage\"},\n",
    "       \"top_k\": {\"type\": \"integer\", \"default\": 10}\n",
    "     }\n",
    "   }\n",
    "\n",
    "4. Backend-Implementierung:\n",
    "   def search_knowledge_base(query: str, top_k: int = 10):\n",
    "       return hybrid_search(query, top_k)\n",
    "\"\"\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEISPIEL: Wiederverwendbare Such-Funktionen\n",
      "================================================================================\n",
      "\n",
      "Query: 'Was ist Plastizit√§t?'\n",
      "\n",
      "1. BM25 SEARCH\n",
      "--------------------------------------------------------------------------------\n",
      "1. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf - Score: 1.89\n",
      "2. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf - Score: 1.81\n",
      "3. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf - Score: 1.79\n",
      "\n",
      "2. SEMANTIC SEARCH\n",
      "--------------------------------------------------------------------------------\n",
      "1. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf - Reranker Score: 3.36470627784729\n",
      "2. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf - Reranker Score: 2.966282844543457\n",
      "3. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf - Reranker Score: 2.5090177059173584\n",
      "\n",
      "3. HYBRID SEARCH (EMPFOHLEN)\n",
      "--------------------------------------------------------------------------------\n",
      "1. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf - Reranker Score: 3.36470627784729\n",
      "2. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf - Reranker Score: 2.966282844543457\n",
      "3. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf - Reranker Score: 2.5090177059173584\n",
      "\n",
      "4. VECTOR ONLY SEARCH\n",
      "--------------------------------------------------------------------------------\n",
      "1. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf - Similarity: 0.7768\n",
      "2. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf - Similarity: 0.6762\n",
      "3. GedaÃàchtnis-in-Multi-Agent-Systemen.pdf - Similarity: 0.6685\n",
      "\n",
      "================================================================================\n",
      "EMPFEHLUNG F√úR AZURE AI FOUNDRY TOOLS:\n",
      "================================================================================\n",
      "\n",
      "F√ºr Production-Agenten empfehlen wir:\n",
      "\n",
      "1. Standard-Tool: hybrid_search()\n",
      "   - Beste Recall-Rate\n",
      "   - Kombiniert BM25, Vektor und semantischen Ranker\n",
      "   - Robust f√ºr verschiedene Query-Typen\n",
      "\n",
      "2. Spezial-Tools:\n",
      "   - bm25_search(): F√ºr exakte Begriffe (z.B. Produktnummern)\n",
      "   - vector_only_search(): F√ºr semantische √Ñhnlichkeit\n",
      "   - semantic_search(): F√ºr nat√ºrlichsprachliche Fragen\n",
      "\n",
      "3. Tool-Definition in Azure AI Foundry:\n",
      "   {\n",
      "     \"name\": \"search_knowledge_base\",\n",
      "     \"description\": \"Durchsucht die Wissensdatenbank\",\n",
      "     \"parameters\": {\n",
      "       \"query\": {\"type\": \"string\", \"description\": \"Suchanfrage\"},\n",
      "       \"top_k\": {\"type\": \"integer\", \"default\": 10}\n",
      "     }\n",
      "   }\n",
      "\n",
      "4. Backend-Implementierung:\n",
      "   def search_knowledge_base(query: str, top_k: int = 10):\n",
      "       return hybrid_search(query, top_k)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Zusammenfassung: Wann welchen Suchmodus verwenden?\n",
    "\n",
    "### 1. Nur BM25 (Abschnitt 7)\n",
    "\n",
    "**Wann verwenden:**\n",
    "- Exakte Begriffe sind wichtig (IDs, Codes, Keywords, Produktnummern)\n",
    "- Schnelle, kosteng√ºnstige Suche ohne Embeddings\n",
    "- Keine Rate-Limits durch Azure OpenAI\n",
    "\n",
    "**Vorteile:**\n",
    "- Sehr schnell\n",
    "- Keine zus√§tzlichen Kosten\n",
    "- Gut f√ºr strukturierte Daten\n",
    "\n",
    "**Nachteile:**\n",
    "- Keine Synonyme oder semantische √Ñhnlichkeit\n",
    "- Nur exakte Keyword-Matches\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Semantische Suche (BM25 + semantischer Ranker) (Abschnitt 8)\n",
    "\n",
    "**Wann verwenden:**\n",
    "- Nat√ºrlichsprachliche Fragen\n",
    "- Verbale Paraphrasen, \"long-tail\"-Queries\n",
    "- Wenn Nutzer unterschiedliche Formulierungen verwenden\n",
    "\n",
    "**Vorteile:**\n",
    "- Versteht Bedeutung, nicht nur Keywords\n",
    "- Besseres Ranking f√ºr komplexe Fragen\n",
    "- Keine Embeddings n√∂tig (kein Rate-Limit)\n",
    "\n",
    "**Nachteile:**\n",
    "- Langsamer als reines BM25\n",
    "- Ben√∂tigt semantische Konfiguration im Index\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Reine Vektor-Suche (Abschnitt 9)\n",
    "\n",
    "**Wann verwenden:**\n",
    "- \"Meaning first\" - semantische √Ñhnlichkeit wichtiger als Keywords\n",
    "- Embedding-optimierte Inhalte\n",
    "- Wenn Keywords fehlen oder irref√ºhrend sind\n",
    "\n",
    "**Vorteile:**\n",
    "- Findet semantisch √§hnliche Dokumente ohne exakte Keywords\n",
    "- Gut f√ºr multilinguale Suche\n",
    "- Robust gegen Tippfehler\n",
    "\n",
    "**Nachteile:**\n",
    "- Rate-Limits durch Azure OpenAI (Embedding-Generierung)\n",
    "- Kann bei sehr spezifischen Begriffen ungenau sein\n",
    "- Langsamer als BM25\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Hybrid Retrieval (BM25 + Vektor) (Abschnitt 10)\n",
    "\n",
    "**Wann verwenden:**\n",
    "- **Beste Allround-Variante f√ºr agentische Systeme**\n",
    "- RAG/Agenten-Workflows\n",
    "- Wenn Sie sich nicht sicher sind, welcher Modus am besten ist\n",
    "\n",
    "**Vorteile:**\n",
    "- Kombiniert St√§rken von BM25 und Vektor-Suche\n",
    "- Robust gegen Tippfehler, Synonyme und Term-Missmatches\n",
    "- Beste Recall-Rate\n",
    "- Reciprocal Rank Fusion optimiert automatisch\n",
    "\n",
    "**Nachteile:**\n",
    "- Etwas langsamer als einzelne Modi\n",
    "- Rate-Limits durch Embedding-Generierung\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Re-Ranking mit LLM (Abschnitt 11)\n",
    "\n",
    "**Wann verwenden:**\n",
    "- Wenn Sie qualitativ sehr hochwertige Top-3/Top-5 brauchen\n",
    "- Wenn weitere Kriterien ber√ºcksichtigt werden sollen:\n",
    "  - Risiko-Bewertung\n",
    "  - Rollen-basierte Relevanz\n",
    "  - Kontext-spezifische Anforderungen\n",
    "  - Compliance-Regeln\n",
    "\n",
    "**Vorteile:**\n",
    "- H√∂chste Qualit√§t der Top-Ergebnisse\n",
    "- Versteht komplexe Anforderungen\n",
    "- Flexibel f√ºr gesch√§ftsspezifische Logik\n",
    "\n",
    "**Nachteile:**\n",
    "- Langsam (zus√§tzlicher LLM-Aufruf)\n",
    "- Teuer (LLM-Kosten)\n",
    "- Nur f√ºr finale Top-K sinnvoll\n",
    "\n",
    "---\n",
    "\n",
    "### Empfehlung f√ºr Production\n",
    "\n",
    "**Standard-Workflow:**\n",
    "1. **Hybrid Search** (Abschnitt 10) als Basis\n",
    "2. Optional: **Semantisches Re-Ranking** (in Hybrid Search integriert)\n",
    "3. Optional: **LLM Re-Ranking** (Abschnitt 11) f√ºr finale Top-3/Top-5\n",
    "\n",
    "**Spezial-Workflows:**\n",
    "- **Nur BM25**: F√ºr exakte Suchen (IDs, Codes)\n",
    "- **Nur Vektor**: F√ºr semantische √Ñhnlichkeit ohne Keywords\n",
    "- **Semantische Suche**: F√ºr nat√ºrlichsprachliche Fragen ohne Vektor-Overhead\n",
    "\n",
    "**Faustregel:**\n",
    "- Unsicher? ‚Üí **Hybrid Search**\n",
    "- Exakte Begriffe? ‚Üí **BM25**\n",
    "- Semantik wichtiger als Keywords? ‚Üí **Vektor**\n",
    "- H√∂chste Qualit√§t? ‚Üí **Hybrid + LLM Re-Ranking**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
